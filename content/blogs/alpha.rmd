---
categories:
- ""
- ""
date: "2017-10-31T22:26:09-05:00"
description: Analysis of TFL Bike Data
draft: false
keywords: ""
slug: alpha
title: "Cycles! - Analysis of TFL Bike Data"
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```



```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(scales)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(countrycode)
```

First, let's see how we import the data using this simple code below. Also within the code you will see a glimpse function that, as the name suggests helps us glimpse through the imported data that is stored in bike.


```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
          
glimpse(bike) 
```
Now, let's look at distribution of bikes hired per month. To do that we will run the below code:

```{r tfl_month_year_grid_reproduced}

bike_1520 <- bike %>% 
  filter(year >= 2015) 

# Creates function for number format on x-axis
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

ggplot(bike_1520, aes(x = bikes_hired))+
  geom_density()+
  # Splits into grid with years as rows and months as columns
  facet_grid(rows = vars(year), cols = vars(month))+
  theme_dark()+
  labs(title = "Distribution of bikes hired per month",
       x = "Bike Rentals",
       y = "")+
  theme(axis.text  = element_text(size = 6),
        axis.text.y = element_blank())+
  scale_x_continuous(labels = ks)

```

If we look at May and June in 2020 relative to the other years we see the standard deviation is higher and the distribution is slightly skewed to the left. Obviously this is because of COVID, where the lockdown have resulted in fewer rentals.

We will now create a graph with four graphic elements into account

* Monthly average calculated throughout the period - this is constant for each month throughout the periods - let us call it mean_bike

* The actual mean value for each month and year, let is call it bikes_hired_mth

* The area between the two values, where mean_bike is greater than bikes_hired_mth, which is colored red

* The area between the two values, where mean_bike is less than bikes_hired_mth, which is colored green

To achieve this, we first calculate the two variables, mean_bikes and bikes_hired_mth, then we create a plot with two lines based on the two variables using geom_line, and then we color the two areas using geom_ribbon.

```{r monthly bike rental}
# First we find the average for each month throughout the whole period
bike_avg <- bike %>% 
  filter(year %in% c(2015:2019)) %>% 
  group_by(month) %>% 
  summarize(mean_bike = mean(bikes_hired)) 

# Now we then find the average for each month
bike_ribbon <- bike %>% 
  filter(year >= 2015,
         day < "2020-08-01") %>% 
  group_by(month, year) %>% 
  summarize(bikes_hired_mth = mean(bikes_hired)) %>%
  # We then join the two data tables
  left_join(y = bike_avg, join_by =  month) 

#Now we then recreate the graph

ggplot(bike_ribbon, aes(x = month, 
                        y = mean_bike,
                        group = 1))+
  # We split the plot into 5 graphs - one for each year  
  facet_wrap(~year)+
  # We create two lines, one for the mean bikes hired per month over the whole period and one for the year in question
  geom_line(aes(y = mean_bike), color = "Dark Blue")+
  geom_line(aes(y = bikes_hired_mth), color = "Black")+
  # We then create two ribbons. The first colors the area at which mean_bike > bikes_hired_mth red, while the second colors the area where mean_bike > bikes_hired_mth green
  geom_ribbon(aes(ymin = mean_bike, 
                  ymax = pmin(mean_bike, bikes_hired_mth)), 
              fill = "Red",
              alpha = 0.2)+
  geom_ribbon(aes(ymin = bikes_hired_mth, 
                  ymax = pmin(mean_bike, bikes_hired_mth)),
              fill = "Green",
              alpha = 0.2)+
 # We finally add titles and change the aesthetics of the graph
  labs(title = "Monthly changes in TfL bike rentals",
       subtitle = "Change from monthly average shown in blue \nand calculated between 2015 and 2019",
       x = "",
       y = "Bike rentals",
       caption = "Source: TfL, London Data Store")+
  theme_dark()
  




```

Let's take this a little further...

```{r weekly bike rental}
# As before we start by creating a dataframe with the average values weekly rental values throughout 2015-2019
bike_avg_wk <- bike %>% 
  filter(year %in% c(2015:2019)) %>% 
  group_by(week) %>% 
  summarize(mean_bike_wk = mean(bikes_hired)) 

# We then find the actual weekly values for each week each year and join the data table we created before
bike_ribbon_wk <- bike %>% 
  filter(year >= 2015,
         day < "2020-08-01") %>% 
  group_by(week, year) %>% 
  summarize(bikes_hired_wk = mean(bikes_hired)) %>%
  left_join(y = bike_avg_wk, join_by =  week) %>% 
  # Finally we calculate the difference from the weekly change in a new column
  mutate(deltapct_bike_wk = (bikes_hired_wk - mean_bike_wk) / mean_bike_wk)



# We now create the graph
ggplot(bike_ribbon_wk, aes(x = week,
                           y = deltapct_bike_wk))+
  facet_wrap(~year)+
  # We add the line showing development during the year
  geom_line()+
  # We use two geom_ribbon to color the area above and below the line. We use the delta as one argument and 0 as the other, as if the actual value = expected value, we would have a delta of 0.
  geom_ribbon(aes(ymin = 0, 
                  ymax = pmin(deltapct_bike_wk, 0)), 
              fill = "Red",
              alpha = 0.3)+
  geom_ribbon(aes(ymin = deltapct_bike_wk, 
                  ymax = pmin(deltapct_bike_wk, 0)),
              fill = "Green",
              alpha = 0.3)+
  # We then add the rectangular fills for the different quarters - first we define the areas we want colored and then we define the colors used
  geom_tile(aes(fill = week %in% c(14:26, 40:53)),
            width = 1,
            height = Inf,
            alpha = 0.25,
            show.legend = FALSE)+
  scale_fill_manual(values = c("white", "gray"))+
  #Now we add the tick marks and color them
  geom_rug(sides = "b", 
           aes(color = ifelse(deltapct_bike_wk > 0, "Above", "Below")),
           show.legend = FALSE)+
  scale_colour_manual(values = c("#7DCD85", "#CB454A"), guide = FALSE)+
   # Finally we fix the aesthetics of the graph
  scale_y_continuous(labels = percent)+
  scale_x_continuous(breaks = c(13, 26, 39, 53))+
  labs(title = "Weekly changes in TfL bike rentals",
       subtitle = "Change from weekly averages  \ncalculated between 2015 and 2019",
       x = "Week",
       y = "",
       caption = "Source: TfL, London Data Store")+
  theme_dark()

```

Here we have used mean instead of median. When deciding on this the main consideration to take into account is how the difference is between the weekend and the week. Intuitively, we would expect every weekday to have fairly similar rental numbers, while the weekends would be either significantly higher or lower. When using median, this would not be taken into account, as one would most likely end with a weekday value (as there are five of them and they are similar). We have therefore included the mean, as we would like to take the weekend number deviations into account.

